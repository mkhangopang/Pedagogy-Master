import { SupabaseClient } from '@supabase/supabase-js';
import { generateEmbeddingsBatch } from './embeddings';
import { extractSLOCodes } from './slo-extractor';

/**
 * WORLD-CLASS NEURAL INDEXER (v190.0)
 * Optimized for Structured Pedagogical Markdown.
 */
export async function indexDocumentForRAG(
  documentId: string,
  content: string,
  filePath: string,
  supabase: SupabaseClient,
  preExtractedMeta?: any
) {
  try {
    const meta = preExtractedMeta || {};
    
    // ATOMIC CHUNKING PROTOCOL
    // We split by "- SLO:" which is the anchor generated by our MD converter
    const chunks = content.split(/\n- SLO:/);
    const processedChunks: any[] = [];
    
    // Header block (the text before the first SLO)
    const headerBlock = chunks[0];
    if (headerBlock.length > 50) {
      processedChunks.push({
        text: headerBlock.trim(),
        metadata: { type: 'header', source_path: filePath }
      });
    }

    // SLO blocks (starts from index 1 because index 0 was the header)
    for (let i = 1; i < chunks.length; i++) {
      const sloContent = "- SLO:" + chunks[i]; // Restore the anchor
      const codes = extractSLOCodes(sloContent);
      
      processedChunks.push({
        text: sloContent.trim(),
        metadata: {
          subject: meta.subject,
          grade: meta.grade,
          slo_codes: codes,
          chunk_index: i,
          source_path: filePath,
          is_atomic_slo: true
        }
      });
    }

    // Sync Vector Store
    await supabase.from('document_chunks').delete().eq('document_id', documentId);

    const BATCH_SIZE = 15; 
    for (let i = 0; i < processedChunks.length; i += BATCH_SIZE) {
      const batch = processedChunks.slice(i, i + BATCH_SIZE);
      const embeddings = await generateEmbeddingsBatch(batch.map(c => c.text));
      
      const records = batch.map((chunk, j) => ({
        document_id: documentId,
        chunk_text: chunk.text,
        embedding: embeddings[j],
        slo_codes: chunk.metadata.slo_codes || [],
        metadata: chunk.metadata
      }));

      await supabase.from('document_chunks').insert(records);
    }

    await supabase.from('documents').update({ 
      rag_indexed: true,
      last_synced_at: new Date().toISOString()
    }).eq('id', documentId);

    return { success: true, count: processedChunks.length };
  } catch (error: any) {
    console.error("‚ùå [Indexer MD-Grid Fault]:", error);
    throw error;
  }
}